<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT911-2389</title>
	</head>
	<body>
		<main>
			<p>910430 FT  30 APR 91 / Technology: The script with double meaning - The struggle to devise a computer standard to cover the world's languages Imagine that your word processor had all the writing scripts of the world at its beck and call. Your invoices to Moscow would be rattled off in Cyrillic characters, your orders to Tokyo would sport beautiful strings of Kanji, and even your personal letters to Paris would have the proper accents. Unfortunately for linguists, today's computers are not up to the task; an Indian computer cannot handle Hebrew any more than a Greek one can master hieroglyphics. Even an American computer is liable to charge a British customer in dollars when it really means pounds - the same coding represents both currencies. For many years now, official standards bodies have agreed that there should be an international system enabling all computers to handle the entire range of graphic characters in use around the world, even if the keyboard was used to access only a small proportion of the characters. Computer suppliers would like this because it could offer them inroads into foreign markets. Until now, such a global system has seemed out of reach. This summer, however, sees the emergence of not one, but two, solutions to the problem. Either system on its own could provide the necessary lead, but if the world ends up with both of them the computer industry could continue in a state of linguistic chaos well into the next century. One of the systems is the work of the International Standards Organisation. Draft international standard 10646 is the culmination of seven years' work by ISO in encoding all the graphic characters of the world. It comes of age in June when 22 countries will finish voting on whether to convert it into an actual standard. But just as 10646 has started its final lap, a rival has run on to the track. Some of the same companies that have worked on 10646 have simultaneously been taking part in another project to develop a simpler alternative which they believe is more commercially acceptable. This system, called Unicode, has already been sent out for evaluation to some 500 computer companies. With the code due to be published in loose-leaf form next month and as a book later in the year, it could soon build up commercial momentum. ISO is therefore in a quandary. If its own standard goes ahead and wins approval, the world may end up with two standards. On the other hand, if 10646 fails to win approval, then by the time it comes round in revised form for another vote, Unicode may be far enough out in front to make 10646 seem meaningless. In an extraordinary move just two weeks ago, the ISO committee that developed 10646 recommended a negative vote in the forthcoming international ballot. Significantly about half of the committee members responsible for the decision were also members of the Unicode consortium. The committee's advice, which will form the basis of the US vote, seems to express a determination not to permit the existence of two standards. However 21 other countries are also voting in June, and many are likely to vote yes. Japan is one of these, because 10646 contains the official Japanese system for classifying characters, unlike Unicode which unifed similar characters from different languages such as Japanese, Chinese and Korean. An impressive array of companies has been involved in developing both 10646 and Unicode. 10646 is the work of a committee comprising representatives from many of the world's leading computer companies including IBM, Xerox, Unisys, Apple, Digital Equipment, AT&amp;T, Hewlett-Packard and Microsoft. Unicode, meanwhile, sprang from a consortium of 22 US companies, again including IBM, Digital, Apple, Microsoft and Xerox. The Unicode companies, restless with the long wait for 10646, started work on their code about two and a half years ago. With the two systems on collision course, both camps are now under intense pressure to compromise, so that the codes could perhaps be merged. At the moment this is seen as almost impossible - technically and practically - yet circumstances are already dictating the need for an extraordinary effort. One of Unicode's strongest advocates is Mike Kernaghan, vice-president of the Unicode consortium. He is critical of the ISO code because he believes ISO is not responding fast enough to customer demand. 'If you look at 10646 from an engineering viewpoint, it would be difficult to build a system around it. We're looking at intelligent people who worked on the code but unfortunately it looks like a camel designed by a committee.' He believes 10646 is so unwieldy that it could never be implemented in full. Instead, sub-sets of the code would be used in different countries. Unicode, on the other hand, could realistically spread around the world, he says. With enough commercial support it could even become an international standard retrospectively, like the current American standard, Ascii. But such optimism may be misplaced. Unicode has a 'snowball's chance in hell' of becoming an international standard according to one adherent of 10646. The 10646 committee would be unlikely to agree openly with this sentiment because it seeks Unicode's co-operation and does not want to be inflammatory. Yet its members suggest ISO would probably find it unacceptable to dismiss seven years' hard graft on the code. Its work has been based on a set of firmly held principles - technical and cultural - and at the moment it cannot see its way to giving them up. But Jerry Anderson, IBM's representative on the 10646 committee, confirms that the overriding concern must be to avoid having two world standards. Recently, he and other 10646 representatives met informally in Paris to discuss tentatively how the two codes might be merged. They hoped, said Anderson, to find a way to make some 'significant concessions and modifications' that would improve the standard and move it closer to Unicode. 'At worst we wanted to accommodate some of the requirements to at least make interworking (between the two codes) easier, and at best to induce a similar flexibility on Unicode's side.' Since that meeting, the 10646 committee has shifted on at least one of the basic technical principles on which it and Unicode differ. Certain codings which were previously reserved for 'control' instructions will be made available for holding graphic characters, as is the case with Unicode. Unicode claims it has made a compromise as well. This involves acceptance of a Canadian idea for a way of merging the two codes by grafting Unicode on to the framework of 10646. The ISO code's framework is so much bigger than Unicode's that it could swallow Unicode in its entirety and still have room to be itself. This idea, floated by Isai Scheinberg of IBM Toronto, could feasibly unite the two codes. 'If they did that we would go along with it,' says Kernaghan of Unicode. But the 10646 committee is having a hard time with the idea. Unicode would have to be recoded for the 10646 framework and, for this, ISO would have to relinquish some of the principles, or restrictions, that are fundamental to 10646. Pressure is on to resolve these problems quickly. Each of 22 countries must vote for or against 10646 by June. To pass, the code needs the approval of 15 countries and less than six must oppose it. At the moment the vote could go either way. Unicode It holds each character as a 16-bit chunk - a series of ones and zeroes 16 units long. There are about 65,000 ways of arranging the ones and zeroes and this means 65,000 characters can be represented using the code - many times more than with present 8-bit codes such as Ascii. It has condensed or unified Chinese, Japanese and Korean symbols into a core of characters from which all the Asian languages can be generated. Two thirds of Unicode's 27,000 listed characters are Asian. It ignores existing conventions for the coding of 'control' characters such as 'carriage return'. It uses 'floating' accents which can be applied to any letter. 10646 It goes beyond Unicode's 16-bit structure to encode data in chunks as big as 32 bits wide. This means that 10646 can potentially store up to 1.3bn characters - far more than it would ever need to. Each Asian language is coded independently after the Japanese blocked an early proposal to unify similar 'ideographs' - ancient Chinese characters. It is more concerned than Unicode about compatibility with existing computers. It avoids codes already used by computers for 'control' instructions. This cuts the available space for graphic characters to 1.3bn from about 4bn. It has separate codes to represent letters in accented and unaccented forms.</p>
		</main>
</body></html>
            
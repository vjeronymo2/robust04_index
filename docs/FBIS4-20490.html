<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FBIS4-20490</title>
	</head>
	<body>
		<main>
			<p>Accelerator   <F P=102>   94P60178A Beijing JISUANJI XUEBAO [CHINESE JOURNAL OF  COMPUTERS] in Chinese Vol 17 No 3, Mar 94 pp 204-211 </F>  <F P=103> 94P60178A </F> <F P=104>  Beijing JISUANJI XUEBAO [CHINESE JOURNAL OF  COMPUTERS] </F>  <F P=105>  Chinese </F> CSO   <F P=106> [Article by Dong Yingfei [5516 6601 7378], Wang Dingxing </F> [3769 7844 5281], et al. of the Dept. of Computer Science and  Technology, Qinghua University, Beijing 100084: ``Architectural  Design of a Fully Connected Scalable Parallel Accelerator,''  supported by grant from State 863 High-Tech Program; MS received  11 May 93]    [Abstract] Current artificial intelligence (AI) computer  systems research platforms fall into two categories: (1)  special-purpose systems (Japan's Fifth-Generation Computers,  Britain's ALICE computer(s), the ESPRIT program's Flagship  computer(s), and the U.S.'s Explorer computer(s)) supporting  symbolic computations and expanding AI applied research and (2)  general-purpose parallel computer systems or  computing-engine-type parallel accelerators (the U.S.'s  hypercube systems and Britain's ZAPP system) designed both for  high-speed numerical computations in parallel C and parallel  Fortran programs and for supporting symbolic computations  through a variety of AI-language compiling systems. As an  example of the latter category, the authors introduce their  fully connected scalable parallel accelerator, a  computing-engine-type scalable multiComputer (smC) system, and  discuss its architecture. The system's processing elements (PEs)  are the INMOS Company's IMS T9000 Transputer, a high-performance  32-bit CMOS RISC microprocessor system with built-in Virtual  Channel Processor (VCP) flow control. The T9000's  interconnection network consists of IMS C104 programmable  32-input/32-output packet-routing switches incorporating  wormhole routing. All of the smC's PEs are fully connected  logically. This parallel accelerator supports multiusers and  multitasking efficiently and is easily scalable.    The basic (32-PE) smC system has a peak operating speed of 5  GOPS (billion operations per second) and 1 GFLOPS (billion  floating-point operations per second) (32-bit). Each PE consists  of a T9000 Transputer with 8 MB or 16 MB local memory. The  T9000's VCP segments the information into packets, with a format  shown in Figure 1, not reproduced. Figure 2, not reproduced,  shows the interconnection network for a single accelerator board  (one-fourth of the basic four-board smC system), consisting of  eight T9000 PEs and three C104 packet-routing switches. Figure  3, reproduced below, shows the interconnection network of the  entire basic parallel accelerator. Figure 4, reproduced below,  is a schematic of a 16-PE parallel accelerator (half of the  basic smC system) showing connections with the system host(s),  Sun SPARC10 workstation(s). Figures 5-7, not reproduced, show a  schematic of the interconnection network for a 16-PE system, the  same for a 64-PE system, and a schematic of the interface  circuitry, respectively. There are no tables.   <FIG ID=JPRS-CST-006-10A>      Figure 3. Interconnection Network of Entire Basic (32-PE) </FIG>   <FIG ID=JPRS-CST-006-10B>      Figure 4. Schematic of 16-PE Parallel Accelerator (Half of </FIG>  References    1. Technical Appraisal Report for Parallel Graph Reduction  Intelligent Workstation [in Chinese], Computer Dept., Qinghua  University, April 1992 [see JPRS-CST-92-012, 18 Jun 92 p 53].    2. The T9000 TRANSPUTER Products Overview Manual, INMOS Doc.  # 72 TRN 228, Order Code: DBTRANSPST/1, 1991.    3. Dally, W. J., Seitz, C. L., ``The Torus Routing Chip,''  JOURNAL OF DISTRIBUTED COMPUTING, 1986, 1 (3): 187-196.    4. Felperin, S. A., Gravano, L., et al., ``Routing  Techniques  for Massively Parallel Communications,'' PROC. OF THE IEEE,  1991, 79 (4): 488-503.    5. Dally, W. J., et al., ``The J-Machine: a Fine-Grain  Concurrent Computer,'' in: Proc. of the IFIP Congress, 1989,  1147-1153.</p>
		</main>
</body></html>
            
<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT942-576</title>
	</head>
	<body>
		<main>
			<p>940628 FT  28 JUN 94 / Survey of Computer Networking (6): Big benefits ahead - Client-server technology / Larger companies see 'downsizing' as a way to cut computer hardware costs The fruits of the collision between computers and telecommunications over the last 20 years have seldom lived up to their promise. Persistent predictions about the imminence of mass teleworking and the global village have always proved premature. The fundamental problem is that getting computers and telecommunications to work together at all is a tough task. Firstly, the two industries are culturally poles apart - telecommunications is slow-moving, heavily regulated and capital intensive, while computing is fast-moving, unregulated and skills-intensive. Secondly, rival suppliers in both industries must be coerced into agreeing to basic technical standards - while leaving them enough room to innovate and differentiate themselves in the market. And standards take years of negotiation, are usually out-of-date or irrelevant by the time they are published and are difficult and expensive to enforce. That it is possible to connect two different computers to a telephone line and get them to swap messages is a modern wonder of the world, against such a background. Even assuming that it is possible to get two different computer environments to talk to each other, the problems of distributing computer power across a network have only just begun. It is not just a case of distributing power by putting workstations on desktops and large local 'server' computers in offices. Modern distributed systems need ways to distribute functionality, too. This is not simple. Decisions must be made about the distributing applications and the difficulties of achieving this are a significant barrier to the spread of client-server technology. The so-called client-server architecture is now widely seen as the way to solve the problem of distributing functionality. It splits information technology (IT) systems into 'client' processes and 'server' processes. Typically, a desk- top workstation incorporating local processing power will be the 'client' and a high-speed laser printer or large data storage device will provide the services through the network. This design approach has been very successful for installing PC-based networks where the design issues are relatively simple - connecting workstation PCs to each other and to printer and data storage resources. But the scope of client-server is now being extended to embrace - and even emulate - traditional central terminal/host applications based on mainframes. This phenomenon is usually called 'downsizing' and it has caught the imagination of large user organisations which see it a way to reduce hardware costs and give them the flexibility to bring their old applications into the 1990s. According to a survey of UK users conducted by client-server database company Gupta, earlier this year, 75 per cent of companies see client-server technology as central to their future IT strategy. Using client-server design principles for 'legacy' applications means that they can be equipped with better 'front end' user interfaces on the desktop. Software environments such as Microsoft's Windows or Apple's Macintosh enable old applications to be presented with a new graphics-based 'front end' and make life easier for users. But genuine client-server systems must go much further than this. Decisions about where to the put the 'logic' of the application - either on the client or the server - are difficult to make. Many companies are ill-equipped for this task. 'One of the main lessons we have learned from large-scale client-server projects is that development people need re-educating and their skills need to change,' says Mr Lawrence Hunt, director of software company ACT's distributed systems group. 'To get the real benefits from client-server technology you need a different attitude to systems design and development. You need to relate more closely to the business needs and get application designers to think about them rather than concentrating solely on the technology,' he adds. There are, of course, technology issues involved and application designers face evergrowing complexity when trying to integrate client-server systems - 'in the past it was the hardware companies that took on the integration role  -but now it is software companies and tool builders,' says Mr Michael Miner, chief executive of Blyth Software, a development tool and database software specialist. 'The problem is that a lot of people have been trained in 1970s technology - much of which is no longer relevant in client-server systems,' adds Mr Miner. Like Mr Hunt of ACT, Mr Miner sees the answer in improving the skills of development staff and placing increased emphasis on business needs. 'Typically, you need a team of people who are skilled with various technologies, but with a focus on the business problem. They need to create new business rules and use tools which make application development easier.' A key aspect of this is to choose tools which can to sort out the dividing line between the client and the server - in other words, whether to place the application logic on every desktop or to place it solely on the server. 'We see the future of client-server in what we call second generation tools,' explains Mr Ketan Karia, director of marketing at software tools builder Cognos. 'Our Axient tool allows developers to build the application code once and then deploy it across different platforms - across databases, operating systems and different approaches to client-server. 'This means that the client can be a 'fat' client with lots of application logic built in or a 'thin' client which relies on logic in the server,' he continues. This approach is likely to become the norm for tool-builders and will help application builders with the difficult problem of dividing client and server components. Even with this particular problem solved, however, client-server systems are likely to be as complex - or, perhaps, even more complex than their predecessors. This will inevitably mean that any savings in hardware costs will be matched by increased software development and system integration costs. The benefits  -more responsive and more flexible systems - should be worth it, however.</p>
		</main>
</body></html>
            
<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FBIS4-20619</title>
	</head>
	<body>
		<main>
			<p>CSO   <F P=106> [Article by Akihide Tachibana and Keji Aoki, Research &amp; </F> Development Div. III]    [Text] An Automated Highway Vehicle System using a computer  vision system has been developed and some test running has been  executed by the prototype vehicle. This paper describes a system  architecture, a white line detection method, a lateral control  method, a longitudinal control method, results of experiment and  technical trends. The white line detection method has high  accuracy and robustness to environmental conditions by using  line edge extraction, a road knowledge model and electric  shutter control. A lateral control method makes use of preview  information (a front white line position data) and is able to  execute a stable automated running at 60 km/h. A longitudinal  control method is able to execute an accurate and smooth  following running by driving force control method and a front  vehicle detection method.  <H5>  1. Introduction </H5>   To realize the safer and more pleasant driving, various  efforts have been made in developing intelligent vehicles. To  cope with the recent large scale and complex problems of the  safety, traffic jams, and other transportation and environmental  protection problems, it is thought that the higher level  intelligence will be required in the future. As one of the  directions toward this objective, an automated vehicle system is  drawing attention and vigorous research and development are  underway (Table 1). With these in the background, we have begun  the development of automated vehicle driving system in  assumption of automated driving on the exclusive lane of a  highway as we thought that it is the closest to the operational  application of the automated driving. Functional requirements  for the automated driving include a lateral control (lane  keeping), longitudinal control, collision avoidance control,  etc. and various methods are proposed for realizing these  functions. Particularly, the running lane detection is the  nucleus of the automated driving technology. Table 2 indicates  the characteristics of the principal detection methods. Of these  methods, the vision method, for example, has disadvantages in  the processing speed and environment resistance (rain, night,  etc.) compared with other methods. Yet, as in the case of a man  who drives by watching some meters ahead, this method enables a  smooth and stable lateral control since it can obtain preview  information. In addition, using the preview information, the  vision method can provide the front vehicle and obstacle  detection functions which are important for the longitudinal  control and collision avoidance control. In the case of other  methods, completely different means are needed, however. Thus  the preview information is the most important characteristic of  the vision method, whose development to a system of higher  performance can be expected with the application of recognition  technology of road geometry, vehicle, and road sign. From the  reason stated above, we have developed the automated vehicle  system that applies the vision (white line) information. This  paper reports on the system architecture, lateral control  method, and the longitudinal control method.       Table 1. Development Examples of Automated Vehicle System   System             Principal of              Lane detection     Driving speed                     development               method   Platooning         University of             Detection of       Special road,                     California-Berkeley       magnetic markers   over 100 km/h                                               with a                                               magnetometer   Convoy-pilot       VW                        Side wall          Special road,                                               detection using    over 100 km/h                                               the laser   PVS                Nissan                    White line         Proving ground                                               detection by                                               vision   MOVER-2            Mazda                     White line         Proving ground                                               detection by                                               vision   VaMoRs             Universitat der           White line and     Expressway                     Bundeswehr Munchen        road boundary                                               detection by                                               vision         -----------------------------------------------------------------------------  |Table 2. Lane Detection Method                                             |  -----------------------------------------------------------------------------  |Item/Method       |Vision            |Magnet            |Side wall         |  -----------------------------------------------------------------------------  |Preview informati-|E                 |_D_             |x                 |  |on                |                  |                  |                  |  -----------------------------------------------------------------------------  |Obstacle detection|o                 |x                 |x                 |  -----------------------------------------------------------------------------  |Processing speed  |_D_             |o                 |o                 |  -----------------------------------------------------------------------------  |Environment resis-|x                 |o                 |o                 |  |tance             |                  |                  |                  |  -----------------------------------------------------------------------------  |Possibility of re-|o                 |_D_             |o                 |  |duction in cost   |                  |                  |                  |  -----------------------------------------------------------------------------  |Possibility of sy-|o                 |_D_             |x                 |  |stem development  |                  |                  |                  |  -----------------------------------------------------------------------------  |E: excellent; o: good; _D_: acceptable; x: not acceptable                |  -----------------------------------------------------------------------------    <H5>  2. System Architecture </H5>   Photo 1 [not reproduced] shows a prototype vehicle for the  automated vehicle system. Using a Crown Majesta for the base,  this vehicle is installed with the hardware shown in Figure 1.   <FIG ID=JPRS-JST-012L-91A>      Figure 1. System Architecture </FIG>    2.1 Computer    A computer (hereinafter referred to as ECU: Electronic  Control Unit) is arranged for each device for the host, image  processing, actuator control, and the sensor processing. With  this architecture, the system has attained the real time  operability through the load reduction from the host ECU, and  the fail-safe measures through the mutual supervision of ECUs.    2.2 Image Processing System    The image processing system consists of a CCD camera and the  white line detection device. It outputs the white line data for  lateral control.    2.2.1 CCD Camera    A compact 2/3 inch CCD camera is installed to the left side  of the inside rear view mirror. It has the lens focusing  distance of 10mm and is mounted at a depression angle of about  24 degrees in consideration of the effect from backlighting and  the vehicle pitching and the white line detection area.    2.2.2 White Line Detection Device    While various methods are proposed for the white line  detection, we have adopted a line edge extraction method for the  present system in consideration of the installability to the  vehicle and reliability. The white line detection under this  method consists of the following two processing:    (1) Edge extraction processing    The monochromatic luminance data input from the CCD camera  is  differentiated using a method called Sobel operator, and the  points having a sharp variation in luminance (for example, the  borderline between the white line and the road surface) in  succession is extracted as the edge. This is processed by the  special hardware at a high speed.    (2) White line recognition processing    In the preprocessing, the edges of the roadside guardrail  and  the buildings are extracted in addition to the white line. To  select the correct white line from this list of line segments,  the following knowledge concerning the white line is used:     -  The detection position of the white line continuously  changes.    -  The road curvature does not change sharply.    -  The white line segment is amply long compared with other  false  line segments.       Through the above-mentioned processing, the white lines on  the right and left hand sides as shown in Photo 2 [not  reproduced] are detected. The processing time required from the  pickup of the image to the output of data is about 300 ms. The  white line position data is output to the host ECU at every 100  ms through the parallel image processing using three sheets of  the board, however, and the lateral control is executed  accordingly. However, since the brightness of the road surface  varies by the weather, time zone, shade, etc. during the actual  running, the electronic shutter speed of the CCD camera is  controlled according to the luminance from the central portion  of the running lane to maintain the robustness to the brightness.    2.3 Steering Drive System    The steering drive system consists of a DC motor, ECU,  steering angle sensor, etc. as shown in Figure 2.   <FIG ID=JPRS-JST-012L-92A>      Figure 2. Steering Driving System </FIG>    2.3.1 Steering Actuator    The steering actuator consists of an electronic clutch and  the reduction gear beside a 3 phase 12 pole brushless DC motor.  The driving performance of the main shaft portion is as follows:     -  Maximum rotation speed: 80 rpm    -  Maximum steering effort: 3.53 N-m    -  Minimum steering angle: 1.67 degrees       2.3.2 Steering ECU    The steering ECU drives the DC motor according to the target  steering angle input from the host ECU. It monitors the steering  angle sensor to control the absolute steering angle and one  steering amount. In addition, it turns the electronic clutch  ON-OFF according to the instruction from the host ECU.    2.4 Longitudinal Distance Sensor    A scanning type laser radar is installed in the radiator  grille. It has a scanning angle of 300R (minimum curve of  highway) and is set for detecting front vehicle running 100 m  ahead.     -  Detection distance: 5 to 120 m    -  Scanning range: +/- 15 degrees (horizontal direction)    -  Beam vertical angle: +/- 0.5 degree (vertical direction)    -  Scanning time: 120 ms/scan   <H5>    3. Vehicle Control Algorithm </H5>   This section describes the lateral control method,  longitudinal control method, and the front vehicle detection  method for vehicle control.    3.1 Lateral Control Method    In the lateral control, compatibility is demanded between  the  high precision lane keeping performance and the direction  keeping performance to a target lane.    Several control methods have been proposed concerning the  steering control.[.sup]1,2[/] We have examined a linear  predictable correction model as shown in Figure 3 in  consideration of the control performance[.sup]3[/] of the  various control models and the capacity of CPU used. To simplify  the control system, the steering drive system has been  structured to respond promptly to the vehicle dynamics and that  with the servo mechanism functioning independently to a target  steering angle. Consequently, for the control system, a PD  controller to compensate for the deviation (lateral deviation)  and the direction (differential of lateral deviation) to the  target lane as shown with equation (1):    _th_ = K(L, V)e[.sub]t[/] + G(L,  V)(e[.sub]t[/]-e[.sub]t-1[/]) (1)    Where:    _th_ = Target steering angle    L = Preview distance    V = Vehicle speed    K(L, V) = Proportional gain    G(L, V) = Differential gain    e[.sub]t[/], e[.sub]t-1[/] = Lateral deviation   <FIG ID=JPRS-JST-012L-93A>      Figure 3. Steering Control Model </FIG>    3.2 Longitudinal Control Method    Important things in the lateral and the collision avoidance  control are correct detection of front vehicle on the own  running lane and the precision maintenance of safe longitudinal  distance from it for the purpose of maintenance of the safety  and the improvement of traffic efficiency.    3.2.1 Front Vehicle Detection Method    To correctly detect the front vehicle and the collision  avoidance object on the own lane, the detection accuracy has  been improved by combining the scan type laser radar with  images. First, the own running lane (white line) as obtained  from the images is converted to the plane  coordinates.[.sup]4[/] And if a point presumed to be a  vehicle from the distance data and the horizontal angle data  from the laser radar should exist inside the own lane on the  plane coordinates, it is judged to be the front vehicle.    Figure 4 shows the front vehicle detection by converting the  white line data on the image picked up from the curve of 150R to  the plane coordinates on which the distance data is laid. In  this case, the lane detection error was maximum 1.2 m in the  longitudinal direction and 0.2 m in the lateral direction.   <FIG ID=JPRS-JST-012L-93B>      Figure 4. Front Vehicle Detection </FIG>    3.2.2 Longitudinal Control Method    To maintain a longitudinal control correctly, a driving  force  control method using the vehicle model was examined. As shown in  equation (2), the target driving force is calculated from the  target longitudinal distance, real longitudinal distance, closed  relative velocity, etc., and the throttle and brake are  controlled according to the value of driving force.    F = M(K(D-D[.sub]t[/])-GV[.sub]r[/]) (2)    F  Mg[.sub]0[/] throttle control    F &lt; Mg[.sub]0[/] brake control    Where:    F = Target driving force    M = Vehicle mass    L = Proportional gain    G = Differential gain    D = Real longitudinal distance    D[.sub]t[/] = Target longitudinal distance    V[.sub]r[/] = Closed relative velocity    g[.sub]0[/] = Coastdown deceleration  <H5>  4. Running Test </H5>   4.1 Result of Lateral Control    Running tests were conducted on the test course including  curves of minimum 80R to examine the characteristics of  respective parameters, and the target control constants were set  up.    4.1.1 Setting of Parameters    First the basic examination was conducted using the  simulation of simple vehicle model on the preview distance L and  the control gain. The control gain was set so that the lateral  deviation e from the target lane in the straight running will be  the minimum value. When the vehicle passed the curve of 80R  under the same gain, the lane keeping performance was maintained  while the preview distance L = 15 to 30 m, but when L = under 15  m, the lane keeping performance was understeer, and when L =  over 30 m, the same performance was oversteer.    Using the above results as precondition, a running test was  conducted at 60 km/h to obtain the target preview distance L and  the control gain. The preview distance L was set for 10, 20, and  30 m. On each preview distance, the gain was set so that the  lateral deviation e on the straight course and the curve (80R)  will be a minimum and the vehicle was run under the automated  steering. Figures 5 and 6 show the result of the testing from  which the following matters have been understood on the preview  distance L and the control gain:     -  When L is made greater, the proportional gain becomes  smaller, and the stability increases.    -  The proportional gains for the straight line and the curve  differ. When L is small, compensation is required by detecting  the road curvature. When L = 20 to 30 m, a stable running  performance can be maintained with a fixed gain.    -  The greater L, the faster the entry of the lane data. For  example, steering starts before entering the curve and the lane  is deviated toward the inner wheel. When L is under 25 m, the  lane keeping performance is maintained.    <FIG ID=JPRS-JST-012L-94A>        Figure 5. Preview Distance and Lane Keeping </FIG>    <FIG ID=JPRS-JST-012L-95A>       Figure 6. Control Gain </FIG>    Judging from the above, the preview distance L should be set  at 20 to 25 m to maintain the steering performance (lane keeping  performance and stability).    On the other hand, the preview distance should be set as  close to the vehicle as possible in the white line detection.  This is for preventing the detection error due to the shielding  of white line by the front vehicle when the preview distance is  long, reflection of the sun light on the road surfaces, and the  thinning of the white line on the image.    From the above, the preview distance L is set to 20 m for  the  present system. Then the control gain is set again by giving  consideration to the balance of the running performance on the  straight course and the curve.    4.1.2 Evaluation of Steering Performance    Figure 7 shows the result of automated running at 60 km/h by  using the preview distance and the control gain as determined in  the preceding section. When compared with the manual steering  (Figure 8) for checking the steering control performance, it was  found that the fluctuation of lateral deviation e was within 20  cm for both automatic and manual steering. This indicated that  the automatic steering has a fairly high level of lane keeping  performance. When the yaw rate fluctuation cycle was checked,  however, the cycle for the automatic steering was short. It was  only several seconds to some 15 seconds for the manual steering.  And the stability was somewhat lowered. This was caused because  the steering actuator was of the step driving type which was  unable to make minute steering and required more frequent  steering correction. This fact is reflected in the steering  fluctuation of these two modes of steering. The fluctuation of  manual steering was +/- 2 degrees while that of the automatic  steering was +/- 4 degrees. Thus, although the automatic  steering was slightly inferior to the manual steering in the  aspect of stability, it achieved such steering performance as  the lane keeping, which was close to the manual steering, by  using a comparatively simple linear predictable correction  model. When the driving speed rises, however, time lag from the  image processing adversely affects the automatic steering,  degrading the lane keeping performance. This shortcoming may be  coped with to a certain extent by adjusting the preview distance  and/or the control gain. It is imperative for the automatic  steering system to increase the image processing speed to  realize stable driving at 100 km/h, however.   <FIG ID=JPRS-JST-012L-95B>      Figure 7. Automatic Drive </FIG>    <FIG ID=JPRS-JST-012L-96A>      Figure 8. Manual Drive </FIG>    4.2 Result of Longitudinal Control    Figure 9 shows the result of following running when the  front  vehicle is driven at 30 to 40 km/h. When the  acceleration/deceleration of the front vehicle was under +/- 0.2  G, it was possible to maintain the longitudinal control with  about +/- 1 m error. When the acceleration/deceleration became  greater, however, detection error occurred because of the change  of the laser radar beam direction due to the pitching of vehicle  body or the target performance failed to be accomplished because  of the delay of response from the throttle and the brake  actuator.   <FIG ID=JPRS-JST-012L-96B>      Figure 9. Following Running </FIG>    Regarding the front vehicle detection, the front vehicle on  the own lane was correctly detected on the flat straight line  and the curve of 150R. When the front vehicle was more than 60 m  ahead, some detection error occurred due to the degraded white  line data accuracy.  <H5>  5. Conclusion </H5>   We have developed an automated vehicle system using images  (white line) as the principal information. We have indicated  that a smooth automatic driving is possible at 60 km/h with the  use of a simple control model.    Since the steering performance becomes degraded under this  control method when the preview distance is made smaller, it is  difficult to achieve stable driving with the deviation data from  just under the magnetic induction cable, etc. Although preview  information is quite effective, it is necessary to improve the  image processing speed and develop a steering control method  using the road information (such as curve curvature, etc.) to  realize stable automated driving at a higher vehicle speed in  the future. To achieve the operational application of automated  vehicle system, it is imperative for us to develop an  environment observation technique that uses the higher image  recognition for the purpose of obstacle avoidance and  coordination driving with other vehicles. Moreover, to realize a  high degree of reliability, it would be necessary for us to have  a composite system with the communications system and  infrastructure in addition to images.    There is a prediction that the automated driving will be  realized at the beginning of 21st century in some form or other.  In either case, there are many technical problems to solve.  Moreover, the automated vehicle system involves a basic problem  of whether the responsibility of driving lies on the driver or  the vehicle. In this connection, the matter requires thorough  discussion so that the social consensus would be reached  concerning the application system and the governing law after  giving consideration to every possible situation that may arise  from the automated driving. On the basis of these matters, we  intend to develop more intelligent automated vehicle system in  the future.  References    1. Yoshimoto: ``Modeling the Driving Behavior of an  Automobile Driver,'' Ningen Kogaku (in Japanese), Vol. 18, No.  6, pp 301-305 (1982).    2. E. D. Dickmanns, A. Zapp: ``A Curvature-Based Scheme for  Improving Road Vehicle Guidance by Computer Vision,'' SPIE Vol.  727, Mobile Robots, pp 161-168 (1986).    3. Shigematsu, Watanabe, Shima, and Ohnishi: ``Automatic  Vehicle Control System Using Driver Model,'' Journal of the  Society of Automobile Engineers of Japan, Vol. 45, No. 2, pp  19-24 (1991).    4. Zen, Sakurai, Kobayashi, and Ozawa: ``Analysis of a Road  Image as Seen From a Vehicle,'' The Transaction of the Institute  of Electronics, Information and Communication Engineers (D),  Vol. J71-D, No. 9, pp 1709-1717 (1989).    5. Taniguchi and Hosaka: ``Technical Trends in Autonomous  Driving Vehicles,'' Nissan Technical Review, Vol. 28, pp 87-93  (1990).</p>
		</main>
</body></html>
            
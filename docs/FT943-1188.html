<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT943-1188</title>
	</head>
	<body>
		<main>
			<p>940926 FT  26 SEP 94 / Survey of Using Computers in Business (21): Progress has been remarkably slow - Client-servers Client-server has been a buzzword in the computer industry for several years, yet progress towards introducing such systems has been remarkably slow, writes George Black. Meanwhile, a backlash against the client-server theory has already taken hold. There are as many plausible definitions of the term as there are suppliers claiming to be experts. Consultancy Butler Bloor's definition seems as good as any: 'The splitting of applications between two or more computers, one of which, the server, serves the others with information interactively.' Many people use the term 'client-server' interchangeably with the older 'distributed systems', even though the latter would seem to be broader in scope. 'Co-operative processing', which enjoyed a vogue for a while, is also very close in meaning. 'Client-server' can be interpreted in many different ways in practice: technologists use 'fat client, thin server' or 'thin client, fat server' to describe different ways of dividing the code between machines. The client-server structure was intended to deliver a much more economical use of computing power than the old mainframes and large minicomputers linked to dumb terminals. These machines were very expensive and often severely overloaded, while personal computers were cheap and not used for much of the time. Mainframes could either be converted to database servers or replaced by smaller machines acting as servers, with PCs taking over much of the application's processing function. For more than a decade now, software houses have been developing tools to enable users to build client-server applications of the same complexity as those which were running on mainframes. But according to Spikes CaveIl, a market research company which tracks activities at UK user sites, although many users are now experimenting in this area few are entrusting their mission-critical applications to client-server systems. 'Right now, vendors would have you believe the trend is further advanced than it is,' comments Mr Luke Spikes, managing director. 'These are small and important developments, but not mainstream ones. If one of these goes wrong, it will not be a major business issue.' Many systems managers are still studying the issue and waiting for the evaluation of pilot projects. They are generally concerned to determine what type of network structure offers the greatest cost-saving without jeopardising technical efficiency. Meanwhile, stories of failed client-server projects and reports that the cost-savings may prove illusory have been deterring users from major investments. In some cases, users have found that much more mid-range hardware was required than they had anticipated; in other cases they could not get the performance required for lack of adequate software tools. Many managers feel it is still too risky to embark on a mission-critical client-server system. Mr Mike Evans, UK managing director of client-server tools developer Gupta, says that many of the failures have been due to lack of experience in building large systems, weakness of project management or confusion over user requirements. However, he argues that the case for choosing a client-server approach has strengthened in the past couple of years because of the introduction of much more powerful server hardware at lower prices. For some, it is the superior user-friendliness of client-server systems which has persuaded them that the risk is worth taking.</p>
		</main>
</body></html>
            
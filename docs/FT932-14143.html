<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT932-14143</title>
	</head>
	<body>
		<main>
			<p>930420 FT  20 APR 93 / Survey of A-Z of Computing (4): Doorway for innovative applications - . . . For Client/Server THE client/server approach to the design of computer systems separates users (clients) and services (servers) and allows them to be distributed across many different computers. It is a flexible design model which not only handles different types of computer and operating environment, but also maintains the level of control required in traditional corporate IT systems. Although centralised mainframe computer systems continue to dominate the operational computing needs of medium-to-large institutions, these systems are limited. They cannot provide graphical user interfaces (GUI) nor can they support innovative applications such as office automation, image processing or multimedia. These applications need special-purpose computing power such as local support for GUIs and remote support for shared databases and printing resources. Advances in computer and communications technology over the last decade have made it possible to distribute specialised computer power to where it is most needed. The client/server design approach provides a general-purpose framework to accommodate these advances and meets the demand for innovative IT applications. It also provides appropriate mechanisms to compensate for the lack of a central, controlling 'authority.' Powerful personal workstations with advanced GUI software and local processing give users a 'window' on to the network. These are, of course, the 'clients.' Specialised 'servers,' which provide services such as shared printing, communications links, special-purpose computation facilities and database storage, can be connected, via a netw ork, to the client workstations. There are significant benefits which come from distributing computer power in this way. Network traffic and communications costs can be reduced while expensive central computers are freed to concentrate on administrative functions such as data integrity, security and systems management. These functions will continue to be important for corporate computer-users and - contrary to some views - point to the survival of central mainframe systems. But instead of their traditional role as the 'engine room' of a whole corporate IT system, mainframes will, in future, act as special 'servers' to the network. The lower cost and technological flexibility of client/server systems opens the door to innovative applications which would be prohibitively expensive or impossible with traditional centralised computing. This includes applications such as image processing, multimedia and voice-mail. These applications demand a different way of looking at IT systems in the light of advances in technology which have made them possible. They use innovative hardware and software which cannot be accommodated within traditional centralized systems. Client/server systems provide the enabling mechanisms for these new applications. They define both the framework and the component specifications to let users build flexible IT systems. The most suitable products can be selected from a competitive, commodity market. This is important with the growth of networks which connect computers from different manufacturers and require that various operating environments work together. Companies are increasingly turning to combinations of different types of computers connected in this way - rather than relying entirely upon a single, central computer. This change - sometimes referred to as 'downsizing' or 'rightsizing' - brings the advantages of flexibility and the client/server approach provides a coherent design model to achieve it. The changeover to client/server systems will not be easy, however. It relies on computer manufacturers and software developers agreeing on a set of standards so that the components fit together properly. This process is, fortunately, now well advanced as a result of initiatives both from suppliers and from international standards bodies, (see 'I' for Interoperability). STANDARDS Most suppliers support the so-called 'open' systems standards which have emanated from groups like the X/Open Group and the International Standards Organisation (ISO). In addition, certain de facto industry standards such as the Postscript printer standard are also widely accepted. The advantage of standards such as these - within the client/server design model - are that they let suppliers compete, while not prejudicing the buyers freedom to choose. The standards only define how clients and servers communicate with each other - not how the various functions are constructed. The most advanced example of this approach is to be found in so-called 'object-oriented' systems where both the software and the hardware components of a systems are defined as 'black boxes.' The Object Management Group (OMG), an industry organisation which promotes standards in this area, has defined ways for computers and software to communicate at a high level. These standards are now widely accepted across the industry and promise to stimulate the move to client/server systems even further. The move to client/server systems will not be easy, however. It will stretch the capabilities of systems builders and try the patience of end-users. But the early evidence from applications based on the client/server model, suggests that the effort is worth it. A report on client/server computing from market researcher Ovum, published last year, records successes from large user organisations as diverse as BP Canada, Hughes Aircraft and Gillette. All pointed to the lack of development tools and inadequate network management. But none of them found fault with the client/server approach and all of them intend to continue to move in this direction.</p>
		</main>
</body></html>
            
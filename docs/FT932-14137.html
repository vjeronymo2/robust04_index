<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT932-14137</title>
	</head>
	<body>
		<main>
			<p>930420 FT  20 APR 93 / Survey of A-Z of Computing (10): Working across the network  -. . . Interoperability THE buzzword 'interoperability' is a necessary compromise. It is the computer industry's solution to the problem of making different computer technologies work together in a network. Interoperability should not be confused with 'open' systems. Although both ideas make use of some of the same technology standards and concepts, they are different. When a manufacturer says its systems can 'interoperate' with other systems, it means that they can exchange data and instructions with other systems over a network. By contrast, open systems work to a common specification. The computer industry thrives on diversity. But this has inevitably led to conflicts between different strands of technological development. The history of industry in general is full of conflicts which arose from different technologies, each competing to achieve the same thing. Usually such conflicts are resolved as one technology emerges as the dominant and the rest, either disappear or are consigned to specialist status. The gauge of railway track, film formats and the VHS video format are examples. Any requirement for competing technologies to 'interoperate' disappears  - because there is only one which is significant. VHS video recorders had no need to include a feature to play Betamax tapes - because no one bothered to use them any more. Computers, however, are different. Even after forty years of use in business, there is still a proliferation of different computer designs and no sign of a single, dominant technology emerging. There is, of course, nothing wrong with this. It allows manufacturers to pursue their own ideas and innovate without the constraints imposed by existing hardware and software technologies. Indeed, it allows them to explore new ways to use computers and acts as a stimulus to progress. Some technologies have, of course, seen off their rivals and created a 'localised' uniformity. The MS/DOS personal computer and the IBM mainframe are two obvious examples. Each has created an industry in its own right and dominates it. But each sector of computing keeps its own idiosyncrasies and speaks different languages - making it necessary for them to 'interoperate.' This poses a significant problem with today's computer networks. Customers now demand systems which combine all of their information resources - regardless of the technology they use. They want to plug personal computers, minicomputers and mainframes together and have them work in harmony. So-called 'open' systems - where the basic computer platform is fixed by agreed standards - are one possible solution. Manufacturers can still pursue their own innovations at the technology level, but open systems standards let them present the same 'face' to the rest of the world. Their computers run the same operating software and application s as everyone else's. Open systems are, at best, a long-term solution. They are fine for new applications and there are many 'open' systems products now available which conform, more or less, to the accepted standards. But computer systems based on 'proprietary' designs, are still needed to support current applications - now often referred to, almost nostalgically, as 'legacy' systems. In the 1990s, however, these systems must be aware of the wider world of the network. No computer can afford to be an island and must, at least, be able to work with others. DIFFERENT APPROACHES Computer manufacturers have adopted a number of different approaches to make their proprietary systems interoperate with others. Some suppliers, like DEC and Hewlett Packard, have pushed their systems increasingly towards the 'open systems' camp by changing their operating software to conform more closely to the standards. Others - such as IBM and Apple - have accommodated other systems within their own proprietary designs. Apple's Macintosh, can for example, exchange data files with MS/DOS personal computers and send standard SQL database calls across a network to get data from a mainframe. IBM's 'interoperability' strategy is, for historical reasons, more ambitious. Not only did it need to provide interoperability with other manufacturers' systems, it also had to bring together several different proprietary designs of its own. In 1987, IBM announced its System Applications Architecture (SAA), a master plan to bring together its personal computers, its AS/400 minicomputers and its System/370 mainframes. In the last six years, IBM has extended the specifications of SAA to include many open and de facto industry standards to let its systems interoperate with others. The purpose of interoperability is well-defined in IBM's SAA. It specifies the operational characteristics, the user interface and the way each component communicates with others. This communication goes beyond the exchange of data between systems. True interoperability must include mechanisms to start programs in other computers. A Postscript program, sent across the network to a laser printer, is a simple example. The wordprocessor program calls up a special program called a 'driver' which generates the Postscript instructions for the printer. In more complex systems, interoperability might extend to the exchange of online transactions (see O for OLTP) across applications. Great progress has been made by manufacturers to provide interoperation features in their products and there is no doubt that the idea will continue to gain ground as more and more computers are connected to networks.</p>
		</main>
</body></html>
            
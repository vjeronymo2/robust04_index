<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT911-3908</title>
	</head>
	<body>
		<main>
			<p>910423 FT  23 APR 91 / Survey of The Computer Industry (31): Consultants come under software attack - Case, engineering approach fails to match expectations THE adoption of traditional engineering principles has helped to improve both the quality of computer software and the process which creates it. Software developers have turned increasingly to the disciplines of engineering to solve the problems which have plagued computer users since the beginnings of the industry in the 1950s. The engineering method, first adopted by the military and academic community in the 1970s, is universally acknowledged - if not universally applied - throughout the computer industry. It has spawned a large and growing market for tools, consultancy and re-training. It has caused businesses to re-evaluate their use of information technology and promised to change the skill needs of the industry. Software engineering makes labour-intensive program coding obsolete and offers greater control over the design and maintenance of software. In spite of significant backing from industry leaders such as IBM and DEC, the engineering approach has not delivered its expected promise. Its supporters face new challenges. The shift away from proprietary, monolithic systems to multi-vendor, networked systems makes it harder to build information technology systems and has increased the pressure on software engineers. Mr Richard Barker, head of Oracle Software's software engineering operations, says this is the biggest problem facing the software industry in the 1990s. 'The problem of designing one logical program to run across multiple hardware platforms will occupy the next few years,' he says. The engineering approach will play a leading role in solving the problem. Until quite recently, software production was haphazard and mysterious. Good results were achieved by a mixture of luck and schooled intuition. However, in the 1980s the engineering analogy grew in credibility and software production was increasingly seen as a craft rather than an art. It was subject to scientific laws and the design and production of a computer program could be defined in the same way that an engineer specifies a bridge or a motor car. This change coincided with increased use of computers as design aids across industry. Computer-Aided Design (CAD) - and the workstation technology it inspired - could be applied equally well to software design and developers quickly discovered how to apply it to their own products. The results were lumped together under the umbrella name of computer-aided software engineering (Case) and packaged up for a market, eager to find ways to build better software at low cost. Software pioneers such as Mr James Martin went as far as to say that the application of formal engineering methods to business systems design not only improved the quality of software, they could also give a business edge in the market. The vision is yet to be fulfilled. For every moderate success, there is equal evidence of failure. Consultant Butler Cox noted in a report, published at the end of the 1980s, that Case tools only worked successfully, where a formal design method - such as Mr Martin's information engineering - had been introduced. In other words, the principles of systems engineering must precede the use of software engineering tools. However, there are other reasons for the apparent slow progress. Mr John Lowrie, who worked with Mr Martin on Information Engineering Facility and now leads Information Architechs, puts some of the blame on consultants: 'The consulting world took over the whole concept of Case and it resulted in complete culture shock in user computer departments. Case changes the skill needs - de-skilling the programming back-end job and re-skilling the front end analysis and design. 'It is a fundamental change and means that companies need a lot of change management - an ideal market for consultants,' says Mr Lowrie. He sees this attitude running against real advances in the software technology: 'Consultants are not interested in producing the best software technology - they just want to put in armies of consultants.' A more compelling reason for the failures of software engineering is that the technological goalposts have moved. Early Case tools were conceived at a time when software was built for a single machine and operating environment - usually a large IBM mainframe or a DEC minicomputer. The information technology systems of the 1990s will be based on networks of PCs, clusters of database 'servers' and high-speed transaction processing computers. 'The current tools were based on the software technology of 1983-84. We are now moving into second-generation products based on late-1980s technology,' says Mr Lowrie. He is cautious about raising expectations too soon. 'I don't think anyone really knows the answer yet - it is very early days. Our view is that we can evolve into other environments because in Case you are maintaining designs not machine code. But it is few years off.' Information Architechs, launched in autumn 1990 as a spin-off from British Gas, says its future developments in Case are aimed at multiple-platform applications. 'We are attracted to the idea that future systems will be based on client/server databases and local-area networks of personal computers,' says Mr Lowrie. Mr John Lewis, vice-chairman of IPSYS, UK software engineering company, is cautious and points to IBM's problems with its Systems Application Architecture (SAA) as an example of the difficulties. 'IBM announced SAA four years ago to provide a model for bringing its three main hardware architectures together. But its hasn't brought out much of it yet. At the same time it has its AIX/Unix developments, which lie outside SAA,' says Mr Lewis. He sees Unix playing an increased role in software engineering, as a development environment and as a platform for applications. IPSYS is working on Case tools to support Unix alongside proprietary environments. 'It is a question of economics. If you set up IBM's AD/Cycle you need an expensive dedicated mainframe and PS/2 for every programmer. The cost is between Pounds 3m and Pounds 4m. With Unix workstations you can set up an equivalent system for about Pounds 1.5m and get far more power for your money,' he says. Oracle says it has plans to extend its Case products so they can produce applications for multi-vendor networks. At the end of this month, Oracle will announce a new generation of Case products which will allow designers to specify the characteristics of the target environment and customise the system. This Oracle says, has implications for users. The company intends to include more features which let the user extend the system and define new objects. The company anticipates that within the next five years the distinction between the development of a system and its live running will disappear. This is a little optimistic. But there is no doubt that this is where software engineering is heading.</p>
		</main>
</body></html>
            
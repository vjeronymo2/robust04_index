<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT942-13145</title>
	</head>
	<body>
		<main>
			<p>940426 FT  26 APR 94 / Survey of A-Z of Computing (4): Increasing power and functionality - Client/Server Systems It makes great sense to put power where it is most effective. This is a fundamental organisational principle and applies as much to computer systems as it does to anything else. During the first 30 years in the history of information technology, however, computer power has been centralised - concentrated on a single computer and doled out to users via terminals. It was far too expensive to distribute computer power. The arrival of cheap personal computers in the early 1980s changed all that. For the first time it became possible to distribute real power to users. But the spread of local power to the desktop - combined with the need to access shared resources - has made old models of IT systems obsolete and prompted a redefinition of how truly distributed information systems can be built. It is not enough to distribute power to the desktop - modern IT systems must distribute functionality, too. Client/server systems make this possible. They bring formal design principles to networks of computers by defining an architectural framework, as well as standards and procedures. The client/server approach breaks an IT system down into sets of processes and mechanisms to allow them to communicate. There are two distinct processes: front-end user processes (clients) and back-end services (servers). Client processes need the power to operate a graphical user interface and perform local data manipulation. Server processes include shared printer servers, communications gateways and common databases. The client/server architecture provides the universal framework and the mechanisms which hardware and software suppliers can use to tie their products together. The idea of client/server systems originated with developments in database technology in the 1970s. It was seen that database operations fell into two distinct operations: making requests for data and physical management. Database software developers split their products into front-end tools for building requests and back-end 'engines' which processed them. They communicated using a language called Structured Query Language (SQL). Initially, both parts of the database ran together on a single multi-tasking computer. But as personla computers replaced terminals at the front end, the two were distributed to PC workstations and server computers. THE WAY FORWARD The network that links the workstations to the servers provides an infrastructure which can be used to distribute other processes and the broader idea of client/server systems evolved from this. It is now widely seen as the future of IT systems. Market researchers agree that the growth of the market for client/server systems will expand quickly and the move to client/server is well under way. The Yankee Group, for example, points to a shift in computer power from larger to smaller platforms. It estimates that PCs and PC LANs will provide more than two-thirds of every corporation's raw computing power by 1995. A survey of 250 of the UK's top 1,000 companies carried out by IDC for the Rothwell Group last year showed that over 40 per cent were either moving or planning to move to client/server systems. Over a third of these tied their decision to wider organisational changes. The IDC survey also gives some indication of the relative immaturity of the client/server market. It notes that the 100 or so companies committed to client/server computing are dealing with 54 different supplier companies. These range from the traditional manufacturers such as IBM, DEC and Hewlett Packard - all of which have placed increasing emphasis on client/server systems in recent years - to software consultancy companies like Hoskyns, EDS, and Logica. Given client/server computing's background, it is no surprise that database software suppliers are taking the lead in the sector. The Yankee Group estimates that the bulk of revenues - Dollars 2.7bn (Pounds 1.84bn) worldwide in 1992 - from client/server products goes to database developers like Oracle, Ingres, Informix, Sybase and Gupta Technologies. The second category, network operating systems software (Novell, Banyan, Microsoft), accounted for a further Dollars 1.5bn in 1992, with tools and other categories following well behind. Database companies are likely to continue as the technology leaders in building the infrastructure of client/server computing. However, the key to successful client/server applications lies with systems integrators. They have to glue all of the technology together and turn it into a useful application. They must build in all of the controls and security which users have come to expect from larger systems and, in addition, keep costs down. They must also determine how applications are split into client and server processes. This is proving to be the most difficult aspect of client/server systems. Ms Christine Comaford, chief executive office of Corporate Computing, a US consultancy specializing in client/server systems, said: 'I have seen Dollars 30m worth of failed client/server systems and many were the result of 'analysis-paralysis'. 'You can't apply old techniques to new technologies and this what designers try to do,' she said. 'It depends on where they come from. If they are into graphical user interfaces (GUIs), they cram everything in the GUI when they should take a layered approach.' Ms Comaford said successful client/server systems must be built on a conceptual model which covers the GUI, the application logic, the business rules and access to the database. This made it possible to realise the full benefit of modern client/server systems, to not only distribute power but also to distribute functionality to the desktop.</p>
		</main>
</body></html>
            
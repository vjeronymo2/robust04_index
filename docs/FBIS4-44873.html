<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FBIS4-44873</title>
	</head>
	<body>
		<main>
			<p>CSO   <F P=106> [Article by Yoshifumi Nagata and Yoichi Takebayashi] </F>   [Text]  <H5>  1. Foreword </H5>   The voice is a natural input system for humans, but a  massive  amount of computation is necessary for speech recognition. It  has not been broadly applied because of problems such as the  framework to integrate the voice input system with the existing  interface not being established. Therefore, based on already  developed high precision software speech recognition  technology[.sup]1[/] applying high speed algorithms, the  problems of integrating the voice input media with existing  input systems was resolved by making the speech recognition  function the server.    The newly developed software speech recognition interface  and  its applications are discussed here.  <H5>  2. Speech Recognition in a Workstation </H5> <H5>  2.1 Making a High Capacity Workstation </H5>   Since speech recognition processing requires a massive  amount  of computation, hardware exclusively for speech recognition such  as the prior digital signal processor (DSP) was necessary, but  the authors used the computation capacity of current general  workstations and developed a software speech recognition system  operating in real time with high speed algorithms.[.sup]1[/]    This system performs high precision speech recognition  responding to specific and non-specific speakers based on the  composite analog method. Recently, other speech recognition  systems with just software for specific speakers have been  developed and an environment in which the speech recognition  function can be used easily has been prepared.  <H5>  2.2 Improvement of Human Interface by Voice Input </H5>   Research on continuous speech recognition is being performed  with the objectives of high precision text input by voice and  dialogue with the computer, but the recognition capacity for  continuous speech is at the level of evaluation in a laboratory  environment and the recognition of isolated words is realistic  when use in a real environment is considered.    Also, in order for voice input to be a standard input system  like the keyboard or mouse, the actual method of the speech  recognition interface is important along with the recognition  capacity, but the framework to integrate voice input with  existing interfaces is not established in the multimedia and  multitasking environment which is generally used in workstations.    Xspeak[.sup]2[/] and CM-SLS (CMU Spoken Language  Shell)[.sup]3[/] are being tried for the speech recognition  interface with the assumption of multitasking. Xspeak can  perform voice input to multitasking in the X-Window environment  (X-Window is software developed at MIT.), but has the limitation  that feedback of the internal state of the applied program to  the recognition equipment is not possible. Also, since CM-SLS is  an interface for use of continuous speech recognition equipment  by means of a network, examination of real time processing and  the form of use on a personal workstation is not sufficient.    The above points were considered, software speech  recognition  technology for which application expansion is easily designed  was employed, multitasking was handled by making the speech  recognition function a server, and improved quality of the human  interface was designed.  <H5>  3. Software Speech Recognition </H5> <H5>  3.1 Software Speech Recognition by the Multiple Similarity </H5> Method    Since the capacity of speech recognition greatly influences  the usage state, this system emphasizes high precision with the  subject being the recognition of isolated words which can  guarantee high recognition capacity.    The function for recognizing single word speech consists of  the acoustic processor and the multiple similarity operator as  shown in Figure 1 (a). It uses the feature vectors attained by  the acoustic processor, performs matching by multiple  similarities with the recognition dictionary, and output the  recognition results.   <FIG ID=JPRS-JST-028L-06A>      Figure 1. Speech-recognition algorithm using the multiple similarity method. </FIG>  In the high speed algorithm (a), speed is  increased by performing the frequency analysis having many  operations only in the word intervals.  &lt;annot&gt; Key: (a) High speed recognition algorithm, 1. Acoustic  processor, 2. Voice, 3. Beginning and ending point detection, 4.  Resample frame determination, 5. Frequency analysis, 6. Word  feature vector extraction, 7. Similarity operator, 8. Similarity  operation, 9. Recognition dictionary, 10. Recognition results,  (b) Prior recognition algorithm&lt;/graphic&gt;     In the acoustic processor, voice input is made at sampling  frequency 8 kHz and quantization 8 bits with the telephone band  CODEC (coding-decoding) contained in a general workstation. The  beginning and ending points of words are detected by the time  change of voice energy. Afterwards, it splits the word intervals  into equal intervals, determines a sample point along the time  axis, performs frequency analysis by a fast Fourier  transformation (FFT), levels the frequency analysis results, for  example it attains filter bank output, and outputs the time  frequency pattern as the word feature vectors. Next, in the  similarity operator, it calculates the multiple similarity of  the word feature vector and the recognition dictionary shown in  the following equation.   <FIG ID=JPRS-JST-028L-06B>      EQUATION </FIG>    Here, S[.sup](i)[/][X] is the category  i similarity value, X is the input word feature  vector, M is the axis number,  _l_[.sup](i)[/][.sub]m[/] and  _l_[.sup](i)[/][.sub]1[/] are the proper values  and _P_[.sup](i)[/][.sub]m[/] is the proper  vector. The above _P_[.sup](i)[/][.sub]m[/] is  found using K-L development from many patterns (word feature  vector). The multiple similarity value of each word is found  with the software and word recognition is performed.    The word recognition discussed above is distinguished by  performing high precision recognition processing entirely with  software and without using any external hardware. In the prior  recognition algorithm according to the multiple similarity  method in Figure 1 (b), convention required dedicated hardware  such as DSP in order for voice analysis since frequency analysis  was performed at all sample points (analysis frame) in the  direction of time. Meanwhile, in the high speed algorithm, the  amount of computations can be greatly reduced since analysis is  performed for only the data resampled at regular intervals in  the voice intervals detected at the beginning and ending points  as shown in Figure 2. With this high speed algorithm, a speech  recognition system with just software can operate in real time  in a multitasking environment.   <FIG ID=JPRS-JST-028L-07A>      Figure 2. Extraction of a word feature vector. </FIG>   The word interval is split into regular intervals, frequency analysis is  made only at resample points set on the time axis, and the word  feature vectors are extracted.  &lt;annot&gt; Key: 1. Power, 2. Threshold value, 3. Start point, 4. End  point, 5. Time change of voice power, 6. Time, 7. Frequency, 8.  BPF output, 9. Channel, 10. Dimension word feature  vector&lt;/graphic&gt;   <H5>  3.2 Improvement of Recognition Capacity by Partial </H5> Abstraction    Since the recognition of word units used in this system can  express the entire and dynamic feature of a word by making a  single pattern of the entire word, high recognition capacity is  attained for the system in which the subject is a non-specific  speaker under noisy conditions. Furthermore, it has the  advantage that it can be applied to foreign languages as well  just by changing the word dictionary.    Also, the word pattern is expressed as several word feature  vectors with different dimension numbers from different  viewpoints due to partial abstraction.[.sup](4)[/] For  example, Figure 3 is the result of extracting several feature  vectors of the word ``clear,'' Figure 3 (a) is the expression of  the entire word, (b) is the detailed expression of the first  half, and (c) is the detailed expression of the second half.  With this partial abstraction, the verification of word voice  from different viewpoints is performed based on the same  multiple similarity; the recognition capacity can be improved by  integrated processing since the trend of error recognition is  generally different in each expression. In particular, the  effect of partial abstraction can be anticipated in the case  where word pairs in which part of the word inscription is the  same are present in the recognition vocabulary.   <FIG ID=JPRS-JST-028L-07B>      Figure 3. Example of frequency analysis and word feature vector. </FIG>   The word pattern is illustrated by several feature  vectors extracted from the different subject sections at  different resolutions.  &lt;annot&gt; Key: (a) Total pattern (16x16 dimension), (b) First half  pattern (12x32 dimension), (c) Second half pattern (12x32  dimension), 1. Waveform (word = ``clear''), 2. FFT analysis  results, 3. Word feature vector&lt;/graphic&gt;     The capacity for non-specific speaker word recognition of  this speech recognition equipment was 98.0% before partial  abstraction and 99.2% after introduction in the case of a  recognition subject vocabulary of 26 words necessary for DTP  system operation. Especially in the case of using the entire  pattern of the word, recognition errors were observed in the  word pairs of ``grouping'' and ``group canceling'' or  ``grouping'' and ``smoothing.'' However, this was greatly  improved with the introduction of partial abstraction. Here. the  partial abstraction was performed using the pattern of the first  and second halves of a 160 dimension word.  <H5>  4. Mounting the Speech Recognition Interface </H5> <H5>  4.1 Application of the Client-Server Model </H5>   The method of interposing the speech recognition driver in  Figure 4 (a) and the method of key emulation in which the speech  recognition results are converted to key input in Figure 4 (b)  are current speech recognition interfaces. Both of these methods  do not presuppose usage in a multitasking environment and have  the problem that changes to the vocabulary which can be input  and changes of the internal state on the applied program side  cannot be fed back into the recognition function.    The authors resolved these problems by making a server of  the  speech recognition function based on the client-server model  shown in Figure 4 (c). This model separates tasks requiring  processing (client) and tasks performing processing (server) and  advances processing by performing communication between those.  The advantage is that the several applied programs which are the  client can have joint resources for speech recognition through  the server and the several clients can use the speech  recognition function.   <FIG ID=JPRS-JST-028L-08A>      Figure 4. Configuration of speech recognition interface. </FIG>  The server type (c) can handle multitasking and performs minute  processing by the reciprocal communication between client and  server.  &lt;annot&gt; Key: (a) Driver type, 1. Voice, 2. Speech recognition  driver, 3. Applied program, (b) Key emulator type, 4. Speech  recognition program, (c) Server type, 5. Speech recognition  server&lt;/graphic&gt;   <H5>  4.2 Multimodal Correspondence Speech Recognition Server </H5>   The trial speech recognition server was mounted on our  company's workstation SPARC (SPARC is the registered trademark  of SPARC International, Inc. in the US.) using the software  speech recognition system discussed above. The speech  recognition server usually operates as one process in the  multitasking environment and makes notification of the  recognition results and recognitions vocabulary by communication  with the client.    Figure 5 shows an example of usage of the speech recognition  server combined with a Window system. Windows A, B, and C are  the clients of the speech recognition server and the voice focus  designating client B as the transmission destination of the  speech recognition results is assigned. For the voice input, the  server uses the recognition vocabulary requested by client B,  performs recognition processing, and transmits the results to  client B.   <FIG ID=JPRS-JST-028L-08B>      Figure 5. Speech recognition server on a workstation. </FIG>  Multimodal input can be effectively performed by establishing  the voice focus and the keyboard focus separately.  &lt;annot&gt; Key: 1. Screen display, 2. Voice focus, 3. Keyboard  focus, 4. Speech recognition dictionary, 5. Speech recognition  server, 6. Acoustic input equipment, 7. Microphone&lt;/graphic&gt;;     The authors connected the two media of keyboard input and  voice input to separate tasks and made possible the control of  several applied programs at the same time through separate input  channels. So that the user can distinguish the separate input  tasks, a task which the user can input is displayed in the  center of the screen as in Figure 5 for example. In this figure,  the keyboard focus is expressed with the thickened window frame  and the voice focus is expressed with the change of window title  color. The voice focus is set with the mouse and voice (window  name). Thus, multimodal input handling the voice, keyboard, and  mouse at the same time can be effectively performed.    With the realization of speech recognition function by  software only and the supposition of the speech recognition  function following the server, the usage of standard speech  recognition without depending on hardware became possible. Also,  under a multi-window environment, voice input for several  applied programs became possible.  <H5>  5. Application of Speech Recognition Interface </H5>   Several application programs have been tried as clients with  the objective of evaluating the speech recognition server. Of  those, the application to the DTP system is discussed as an  example. The DTP system has a screen as in Figure 6 (a) and  performs editing of text and images. The DTP control is  performed by converting and transmitting the speech recognition  results to keyboard commands with the control program shown in  Figure 6 (b).    Keyboard commands use the API (Application Program  Interface)  provided by the DTP system. The recognition vocabulary uses the  menu tier of the DTP system and is limited and the recognition  capacity can remain high. The route of the menu tier is called  the ``top level;'' words from the top level are spoken and the  commands are executed by going through the menu. At each  movement in the menu tier, the menu item in the window and the  current position in the menu tier are expressed in the form of a  path and presented to the user.    The operation target is selected with the mouse and the  operations such as ``cut,'' ``paste,'' and ``up/down reverse''  for the target are performed by voice. Since these two media can  share roles in a natural form, complex mouse operations during  text editing and figure editing are reduced and an interface  with good usage conditions can be realized.  <H5>  6. Conclusion </H5>   A standard speech recognition interface for a workstation  multimedia/multitasking environment was developed. This speech  recognition interface can handle multimodal input including  keyboard, mouse, and voice input by making a server of the  speech recognition function. Also, with the high precision  software speech recognition technology performing processing in  real time, a wide range of applications which do not rely on  hardware are possible. In the future, research is planned with  the objective of establishing a human interface with better  usage conditions and using voice media.  Footnotes    1. Y. Nagata, et al: Development of the speech recognition  function in a workstation, Denshi Joho Tsushin Gakkai  Kenkyukaishiryo, HC-9119, pp 63-70 (1991).    2. C. Schmandt et al: Augmenting a window system with speech  input, Computer, No. 23, pp 50-58 (1990).    3. A. Rudnicky, et al: Spoken language recognition in an  office management domain, Proc. Int. Conf. Acoust. Speech &amp;  Signal Process, S14.7, pp 829-832 (1991).    4. Y. Takebayashi, et al: Keyboard spotting by partial  abstraction of word feature vectors, Denshi Joho Tsushin Gakkai  Onsei Kenkyukai, SP91-104 (1991).</p>
		</main>
</body></html>
            
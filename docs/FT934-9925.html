<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT934-9925</title>
	</head>
	<body>
		<main>
			<p>931108 FT  08 NOV 93 / Survey of Computers in Finance (12): Lessons learned the hard way - Advice on ways to get back to business after a disaster WHEN terrorist bombs first hit the City of London, few had contingency plans. A second bomb in the past year made companies look more closely at how they would operate if the computer centre was out of action for any reason. The Financial Times talked to some of the companies affected by these bombs about the lessons they have learned. Below, we summarise their main recommendations, along with advice from disaster recovery experts: Store data off site: Mary Jackson, head of technology support for National Westminster Bank in the City, believes it is important to store data off site. Because the Bank moved Friday night's Wang back-up tapes out of the NatWest Tower early the following morning (as usual), it did not lose any data when the second City bomb went off on the morning of Saturday, April 24. Opt for smaller systems: Ashley Duran, former data processing manager at the Baltic Exchange (which was hard hit by the first bomb at St Mary Axe) says: 'The lesson I've learned is that if you upgrade to (small) modern machines, you can pick them up and move them in a crisis. They aren't site specific like mainframes.' The Exchange's mainframe was 'eight years old and 18 foot long and would have cost three times' more to move than it was worth,' he says. Add telecom lines: Brian Stacey, technical operations manager at stockbroker James Capel says the first bomb 'made us realise just how vulnerable we are to telecoms links. BT and Mercury were very helpful but getting a switched circuit is one of the hardest things to turn round in less than 24 hours. 'One of the constraints at our Devonshire Square office, in which we set up a temporary dealing room, was getting a link to the Stock Exchange. In hindsight, we had under-provided for spare telecom circuits.' Since then, the company has spent more money on spare capacity to invoke if and when needed, and on alternate routing. Allow time for access to buildings: Andrew Hiles, chairman of the disaster recovery user group, Survive], says it is important to allow time for access to company premises when designing a contingency plan. After the City bombs, some companies could not get into their premises for 24 hours or more because the police had cordoned off the area for safety reasons. Plan - and test the plan: Carole Allen, marketing manager of IBM's Business Recovery Service, says: 'Testing disaster recovery plans is essential, but rarer than you might think.' A 1993 IBM survey shows that only 57 per cent of UK organisations have a formal written contingency plan and, of these, only 65 per cent had bothered to test it. Even then, only 22 per cent could be considered viable because they had been regularly updated and tested (ie at least once a year). The survey also shows that fewer than one-in-three organisations include personal computers in their recovery plans, despite the fact that PCs are increasingly integrated into the business and perform business-critical functions. Buy as much contingency as you can afford: Mr Stacey advises companies to 'have as much contingency in place as you can afford.' James Capel did not have a contract with a disaster recovery company when it was affected by the first bomb, but Mr Stacey says if it had not had a second office a few hundred yards away (at the time) and a second Tandem computer (for development work) located there, 'We'd have had to consider that.' His view is that 'all contingency is a trade-off between risk and cost.' Arrange an alternative site: Arranging alternative sites is a critical part of any disaster recovery plan. Andrew Hiles says companies considering such a plan should draw up a list of facilities available locally to each division including the availability of 'hot sites' (sites fully-equipped with computers and available at short notice) and 'cold sites' (empty rooms, perhaps with air-conditioning, in which computers can be installed). They should also look at the communications facilities available. Once the list of resources has been drawn up they should consider ways in which they can improve their resilience. For example, because of different time zones it may be feasible to use the company's US computer centre as a back-up if the UK computer centre is out of action (or vice versa). However, a detailed plan for, say, making sure that the US centre always has a copy of the latest back-up tapes and software would have to be worked out. If a company cannot provide adequate back-up using in-house sites, it should consider signing up with a third-party disaster recovery company, or a computer company which has moved into the recovery business. Third-party suppliers offer a range of services including: hot and cold stand-by sites, back-up computers (sometimes provided on-site in a mobile articulated lorry), help with designing and testing disaster recovery plans, and staff training. Suppliers include Comdisco, Datashield and CAP-RS (specialising in large systems recovery) and SafetyNet and CDRS, which focus on the mid-range market. Computer manufacturers, such as Digital Equipment, Hewlett Packard, IBM and ICL, also offer these services for their own and, in some cases, competitors' systems. The cause of computer disasters varies. According to Survive], fraud, malice and misuse account for 30 per cent of disasters; software and hardware failures for another 30 per cent; 'acts of God,' such as fire, flood and earthquake account for 20 per cent; user-error causes 11 per cent; and power-loss, 9 per cent. 'Survive]' is holding a one-day Business Continuity Workshop in London on November 23. More information on (UK): 081-871 2546.</p>
		</main>
</body></html>
            
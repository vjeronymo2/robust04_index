<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT924-15216</title>
	</head>
	<body>
		<main>
			<p>921006 FT  06 OCT 92 / Technology (Technically Speaking): Insuring against software risks COMPUTER software sometimes seems to defy common sense. To ensure the safety of a load-bearing beam, an engineer typically doubles the thickness. Applying the same approach to a piece of software simply multiplies the risks of error. The question of how safety and reliability can be built into computer software is increasing in importance as critical social and industrial processes - from the control of aircraft and power stations to financial planning - yield to computerisation. This dissemination of computer power will accelerate over the next few years because of the growth of international computer networking. Orders, invoices and payments will increasingly travel digitally across continents and arrive as electronic documents. The painstaking care needed to write fault-free programs can be difficult for non-specialists to comprehend. But although there is widespread awareness of the consequences of software errors, there is also a reluctance among business managers and legislators to make greater efforts to understand the way software is created and its limitations. This reluctance manifests itself in a number of different ways. There is, first, the question of whether software can be relied on not only to do what is intended, but also to refrain from doing what is not intended. An example is the issue of 'phantom' withdrawals from banks' automated teller machines. The banks' argument has been that their machines are infallible. A cursory glance at research into software dependability should have convinced them that such claims are very difficult to sustain. When software systems reach a certain size, there are simply too many variables in play for all the potential consequences to be tested. The Stock Exchange's Topic software failed during the first day of Big Bang six years ago because of a fault which had lain undetected for years. Second, there is the question of resistance to outside interference, whether intentional or not. Breaking into a computer system from the outside may seem much less probable than an inside job, but the volume of fraud related to computer systems is no longer insubstantial. All the signs are that it is an appreciable amount. But businesses are still reluctant to expose the fragility of their computer systems by reporting fraud, so the full extent of the damage is hard to measure. This has the unfortunate side-effect of making actuarial assessment of the risks difficult to quantify. Increasingly it may be necessary both for companies and individuals to insure against the consequences of computer fraud and failures. The establishment of an actuarial database to help determine premiums would be a step in the right direction. Finally, when software has to be modified (and all major software has to be modified or maintained), can the work be carried out economically, in the time allowed and without introducing errors in other parts of the system? The struggle most importers and exporters face to make their accounting software ready to accommodate changes in Europe-wide VAT regulations next year is just the latest in a series of computing fiascos caused by a lack of understanding on the part of senior managers of just how long it takes to make changes to established computer systems or develop new ones. A danger is that pressures of time or resources might tempt software developers to cut costs and corners. There are a number of pieces of research into safe software, including the European Commission's Predictably Dependable Computer Systems project, but so far there have been no easy answers. Until there is a breakthrough, awareness of the risks and a willingness to insure against them seems the best defence.</p>
		</main>
</body></html>
            
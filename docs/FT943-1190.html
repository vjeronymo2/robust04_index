<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FT943-1190</title>
	</head>
	<body>
		<main>
			<p>940926 FT  26 SEP 94 / Survey of Using Computers in Business (20): Invented to meet a need - George Black reviews developments in interoperability Interoperability is among the ugliest words coined by the computer industry, but like many of its kind it was invented to meet a need. It means something more than either compatibility or portability. Systems can be compatible with each other without working together smoothly. Software can be ported from one machine to another without allowing applications to communicate effectively. UNIX systems, for instance, have gained popularity partly because they are more portable than the older proprietary systems, but this does not produce instant interoperability. XjOpen, the international promoter of open systems, is about to produce a single specification for UNIX (known as Spec 1170) replacing existing incompatible versions. This will improve portability, but will leave the interoperability issue not solved. The current X/Open brand, displayed by products which incorporate its XPG4 specifications, is a certificate of portability, not of interoperability. X/Open plans to launch an interoperability brand during 1995, working more closely than before with the Open Software Foundation consortium, previously a competitor. The brand is likely to draw heavily on OSF's DCE (Distributed Computing Environment) software. For the past few years interoperability has been one of the main goals of users, vendors and standards bodies alike. At its technical briefing this summer X/Open stated that it would continue to be one of its main goals for at least the next three years. Some industry observers have the impression that interoperability is a continually receding goal, but XjOpen insists that it is now in sight and attainable. Progress is slow because it depends on getting consensus among both vendors and users. It can take 18 months from agreeing a requirement to publishing a specification and a lot longer for vendors to market compliant products. The complexity of testing new products creates a bottleneck and there is a serious shortage of money to do all the work. Mr Mike Lambert, chief technical officer of X/Open, defines interoperability in non-technical terms as 'the ability of two different applications to do something useful together.' He says the main problem is not a technical one but concerns the way in which computer systems are delivered, integrated and supported. 'Banks and government departments have the systems integration skills in-house to cope with this, but many smaller users do not,' he says. Interoperability depends on products complying with de jure and de facto standards. That, however, begs a number of questions, most importantly what constitutes a standard. A couple of years ago X/Open intended the Open Systems 1nterconnection (OSI) model of the International Standards Organisation (ISO) to be the basis of its brand. It encouraged migration from unofficial protocols such as TCP/IP (Transmission Control protocol/Internet Protocol) to official OSI standards. But Mr Lambert acknowledges that the whole industry's emphasis has moved away from de jure to de facto standards, such as TCP/IP. Earlier this year, the Brussels-based standards Promotion and Applications Group (SPAG) was forced to abandon its OSI interoperability testing programme because demand for OSI products was not enough to finance the venture. Its work may be taken over by X/Open. Mr Lambert concedes that at the lower levels of communication OSI has failed in the market, but argues that at the higher levels affecting interoperability it has been successful. He cites the X400 messaging standard as an example.</p>
		</main>
</body></html>
            
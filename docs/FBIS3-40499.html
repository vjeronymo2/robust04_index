<!DOCTYPE html>
<html lang="en-US"><head>
		<meta charset="UTF-8">
		<title>FBIS3-40499</title>
	</head>
	<body>
		<main>
			<p>on   Aircraft Computational Aerodynamics  <DATE1>  7 February 1994 </DATE1> <F P=100></F> <H3> <TI>     Selections from the Proceedings of the 9th NAL Symposium on Aircraft Computational Aerodynamics </TI></H3> <H4>   Numerical Wind Tunnel; Requirements on the Outline </H4> <F P=102> 93FE0499B Tokyo SPECIAL PUBLICATION OF NAT'L AEROSPACE  LABORATORY in Japanese Dec 92 pp 91-97 </F>  <F P=103> 93FE0499B </F> <F P=104>  Tokyo SPECIAL PUBLICATION OF NAT'L AEROSPACE  LABORATORY </F>   Language: <F P=105>Japanese </F> Article Type:CSO   <F P=106> [Article by Hajime Miyoshi of the National Aerospace </F> Laboratory]    [Text] Abstract: In this paper, it is shown that  the Numerical Wind Tunnel (NWT) with an actual performance of  more than 100 times higher than Fujitsu-VP400 is feasible using  a multi-computer architecture and crossbar interconnection  network.  <H5>  1. Introduction </H5>   Let me tell you what I did at last year's 3d aircraft  computational aerodynamic symposium. I proposed:    (R1) That it is necessary to quickly develop a high-speed  computer which will exhibit at least 100 times the CFD program  processing speed of the Fujitsu-VP400 (hereinafter called simply  the VP400), with a main memory area of 32 GB or so for data  storage, and to use this in the NAL computer system.    This would be for the purpose of exploiting the merits of  numerical simulation technology, for the purpose of employing  the CFD gains made thus far in the aircraft aerodynamic  development which is beginning to be done in Japan, and to push  CFD to even higher levels. I also showed that it is possible to  quickly implement this computer by adopting parallel computation  techniques.[.sup]1[/]    In this paper I press last year's argument even further and  report on:    (1) the considerations that form our starting point for  studying the idea of a numerical wind tunnel,    (2) the conditions necessary to produce a practical computer  exhibiting the performance noted in (R1),    (3) the hardware configuration for a computer exhibiting the  performance noted in (R1), and    (4) a summary of the functions required in program writing  and the compiler.    Furthermore, with the purposes for developing a computer  that  will satisfy the performance demands noted in (R1) firmly in  mind, in the remainder of this paper I consider that computer to  be a numerical wind tunnel (abbreviated NWT).  <H5>  2. NWT Concept Starting Point </H5>   In this section I will briefly discuss some aspects of the  starting point for the NWT concept. The details are discussed in  the References.[.sup]2[/]  <H5>  2.1 NWT Approach </H5>    (1) NWT Will Be Distributed Main Memory Type Parallel  Computer    In terms of how the main memory is structured, we may divide  parallel computers into those which employ common main memory  and those which employ distributed main memory (cf. Figure 1).  We decided to make the NWT a distributed main memory parallel  computer for three reasons, namely (1) the fact that there are  few limitations on improving raw computer processing speeds, (2)  the fact that there are great possibilities for improving  performance in the future by incorporating advances in element  technology, and (3) the fact that the big problem with  distributed main memory type parallel computers -- i.e. the  problem of addressing main memory -- can be to some extent  resolved, we now think, by incorporating virtual common main  memory space.[.sup]3[/]    <FIG ID=JPRS-JST-002C-05A>           Figure 1. Distributed Main Memory Parallel </FIG>      (2) Element Computer (abbreviated PE) Will Be Vector  Computer  Equipped With Vector Registers (abbreviated VR)    We have learned from our experience to date with vector  computers that VR-equipped computers having chaining functions  are suitable for CFD programming. The Fortran compiler  technology for this mode of computation has reached a high  level, moreover, and so long as the hardware has the standard  hardware functions for vector processing, the vectoring ratio  for CFD programs is 99% or thereabouts. That is why we  decided  that the PE would be a VR-equipped vector computer.  <H5>  2.2 Preconditions for CFD Programs </H5>   Paralleling and Vectoring Ratios Reaches 99% and  Higher in  CFD Programs.    Most of NAL's CFD programs now attain a vectoring ratio of  99% or higher. If the degree of paralleling is determined  in  line with the scale of the problem, the paralleling ratio of CFD  programs also reaches 99% and above. Both of these ratios  rise  as the scale of the problem increases relative to the degree of  paralleling.  <H5>  2.3 Usable Hardware Elements </H5>   (1) Theoretical Elements    Recent trends in theoretical element (device) speeds are  plotted in Figure 2. The gate switching delay time is plotted on  the vertical axis and the year on the horizontal axis. In Figure  3 are plotted the trends in theoretical element densities, with  gate density per chip plotted on the vertical axis and the year  on the horizontal axis. Based on these two figures, and on  recent theoretical element per-chip power consumption trends, we  expect that the following three devices can all be used as NWT  devices.    <FIG ID=JPRS-JST-002C-06A>           Figure 2. Theoretical Element Speed Trends </FIG>      <FIG ID=JPRS-JST-002C-06B>           Figure 3. Theoretical Element Density Trends </FIG>      (i) 30,000 gates/chip: Switching delay time 50 ps  (10[.sup]-12[/] seconds); Power consumption 40 W/chip ECL  gate array    (ii) 100,000 gates/chip: Switching delay time 350 ps  (10[.sup]-12[/] seconds); Power consumption 20 W/chip BiCMOS  gate array    (iii) 200,000 gates/chip: Switching delay time 600 ps  (10[.sup]-12[/] seconds); Power consumption 10 W/chip CMOS  gate array    (2) Main Memory Elements    As to the main memory element, the 4 M/chip SRAM is  beginning  to be mass-produced. Accordingly, it is possible to use either a  4 M/chip or 1 M/chip SRAM as the main memory element and  implement a main memory of 30 to 40 GB.[.sup]1[/] Which chip  to go with will depend on cost and speed considerations.  <H5>  3. Conditions Needed for Implementing Numerical Wind Tunnel </H5>   In the introduction we noted that the performance goals for  the numerical wind tunnel have been established, but the answers  forthcoming from computer engineering do not give a clear  picture of how this can be implemented. There are many technical  decisions which have to be made.    We discuss briefly here some conditions that will provide  judgment criteria when making technical decisions.  <H5>  3.1 NWT Cost </H5>   When we look at the numerical wind tunnel from the  perspective of R&amp;D on aerodynamic technology for aircraft,  the  numerical wind tunnel is a piece of aerodynamic testing  equipment that complements the conventional wind tunnel. If the  cost is too high, therefore, the NWT cannot be used in  parametric studies and the like for the purpose of doing  aerodynamic design of aircraft utilizing the advantages of  computers.    Accordingly, if we are to implement the NWT, then we must  look beyond performance alone, and do our best to lower  development and production costs and operating costs. When  considering such costs, naturally we must think about the cost  per data set of each wind tunnel test. In this day and age we  must consider the cost of data as well as the reliability of  data for numerical simulations based on the RANS equations.    This means that it will be very difficult to implement the  NWT by linking up dozens of supercomputers.  <H5>  3.2 Reliability </H5>   The improvements in recent years in both element technology  and mounting technology have had an enormous impact in enhancing  the reliability of computers. It is nevertheless a well known  fact that devices adopted for improved reliability often have a  negative impact on performance improvement and cost reduction.    With a numerical wind tunnel, the volume of materials used  will probably become enormous, but what would be appropriate for  the purpose of improving reliability and maintainability? The  users of the numerical wind tunnel will be CFD researchers and  technicians, rather than computer engineering specialists, so a  stringent attitude will probably be taken towards malfunctions.  Furthermore, since the numerical wind tunnel is not just an  experimental computer but also a practical computer, it will be  just as important -- in terms of the success or failure of the  numerical wind tunnel -- to maintain NWT hardware reliability at  high levels as to achieve the targeted performance in the first  place.  <H5>  3.3 Performance Demands for Element Computer </H5>   Two items in particular will be necessary in order to take  the NWT, which is a distributed main memory parallel computer,  and achieve its widespread use among ordinary operators who are  not computer specialists, as follows.    (i) Either an optimizing compiler for Fortran programs  written according to a Fortran standard for parallel  programming, or a compiler that automatically optimizes programs  written in current Fortran for parallel implementation.    (ii) Parallel-running programming support software including  debugging and tuning tools.    We will bring a practical computer in among CFD technicians  and researchers who are not computer specialists and seek to  devise both (i) and (ii) above in conversations with computer  specialists and ordinary users. So doing, we will adopt a policy  of promoting the growth and proliferation of knowledge needed  for high-level utilization of parallel computers among users.    When this policy is adopted, it is necessary to guarantee  the  following minimum limitation for users.    (R2) The processing of jobs that are appropriate for  processing using the VP400 are to be processed on one element  computer without program alteration, and the processing speed  therefore is to be the VP400 speed or higher.  <H3>  3.4 Conditions for Interconnecting Network -- Compatibility  Between Interconnecting Network and CFD Computation Methods </H3>   When we survey recent CFD computation techniques from the  perspective of parallel computing, we see the following.    (1) Computation schemes are tending to become more complex  to  achieve higher precision.    (2) Research is being done on non-structure lattices as well  as on structure lattices.    (3) In terms of solving methods, negative solution methods  in  general, and the IAF method in particular, are dominant over  positive solving methods, but if interest shifts to problems  such as unsteady problems where there is not much of an  advantage with the negative solving methods, the present  situation could change. Nevertheless, when it comes to  applications to actual problems of technological development,  the track record of the IAF method cannot be disregarded.    Item (1) will make stringent demands on the performance of  parallel computers in the area of interconnecting network data  transfers.    In processing CFD programs which use the non-structure  lattices noted in item (2), list vector processing will be  mandatory. This will make stringent demands not only on  interconnecting network data transfer performance, but also on  hardware and software performance.    In methods for solving linear equations that are negative  solving methods, it will be necessary to adopt sequential  computation for single-axis directions in three-dimensional  space.    With structure lattices which are graphically simplified, it  is not practicable to disregard the computation methods that  employ sequential computations in single-axis directions, so,  when one axis is used in vectoring, the remaining axis will be  parallelized.    When we consider that the scale of problems visualized for  the NWT having the power set forth in (R1) is on the order of  several thousand x several hundred x several hundred, this comes  bouncing back in the form of a limitation on the number of  parallel computers.    In particular, with the IAF technique that is an important  negative solution method, the sequential computation axis  changes from X to Y to Z, so not only are stringent performance  demands made on interconnecting network data transfers, but  stringent demands are also made on the network topology and on  the number of element computers configuring the numerical wind  tunnel.    Our numerical wind tunnel must be able to deal effectively  with all CFD techniques. And we should not adopt an  interconnecting network which exhibits high performance only for  specific CFD techniques and inadequate performance for other  techniques.    From (R2) set forth in 3.3 above and the conditions noted in  this subsection, we can see that it will be very difficult to  implement NWT with several thousands to several tens of  thousands of microprocessors.  <H5>  4. NWT Hardware Configuration, Performance </H5> <H5>  4.1 PE Configuration, Performance </H5>   The processing speed of a VR-equipped vector computer is  determined mainly by the following eight factors.    (i) Machine clock cycle time (hereinafter abbreviated _t_)    (ii) Pipeline multiplicity    (iii) Type and number of pipelines, and number of pipelines  supporting simultaneous activity    (iv) Ability of main memory to provide data    (v) VR number and length    (vi) Rise time for various pipelines    (vii) Instruction execution control format    (viii) Scalar instruction processing mode, register  configuration, cache memory volume and control mode.    First, we assumed that items (vii) and (viii) would be the  same as in the VP400, and then prepared many PE candidate models  in terms of PE design parameters for the hardware resource  quantities and timing noted in items (ii) through (vi). Next we  examined typical CFD programs at NAL and selected 18  representative characteristics including number of vector  instructions, ratio of various types of vector instruction, and  the ratio between vector load/store instructions and vector  operation instructions and then estimated 18 DO-loop processing  speeds for various PE models, with vector lengths of 32 and 128,  using the software simulator VTAP which simulates the actions of  a vector computer. The error in the VTAP simulations for the  VP400 processing speeds for the 18 DO loops was within +/-  10%  and the mean error was 6.8%.    In Figure 4 we have plotted the results for one PE model  series. The pipeline multiplicity in this PE model series was  varied from 1 to 2 to 4 to 8 to 16, with the other parameters  being made up of six fixed models.    <FIG ID=JPRS-JST-002C-08A>           Figure 4. PE Pipeline Multiplicity (128 VRs) </FIG>      Pipeline multiplicity is plotted on the horizontal axis in  this figure, and machine clock time _t_ on the vertical axis.    Here we modify the (R2) demand that the PE CFD program  processing speed be VP400 or better to (R2').    (R2') VTAP estimated value for PE processing speed for all  18  DO loops at vector lengths of 32 and 128 &gt;  VTAP estimated  values for VP400 processing speed.    The times in Figure 4 of 1.6, 3.1, 6.4, 12.6, and 12.9 ns  are  the required PE machine clock times as calculated from the  simulation results for maximum machine clock times at which the  PE model satisfies (R2'), and then shortened another 10%  in the  interest of safety. The ECL and BiCMOS in the diagram are the  names of the logic elements that should be adopted to achieve  these machine clock times. The x symbol indicates that it is  difficult to achieve this value of _t_ by means of usable logic  elements.    After doing comparative studies on the results of  simulations  on various other PE models (besides this PE model series), we  found that this PE model series is the most effective series  among the PE models which we studied.    After comparing the power consumption of a PE with _t_ =  3.1  ns and a pipeline multiplicity of 2 using ECL elements with that  of a PE with _t_ = 12.6 ns and pipeline multiplicity of 8 using  BiCMOS elements, we found that the latter PE model would be  better for use in the NWT. When BiCMOS elements are used,  moreover, one can be sure that a machine clock time of around 10  ns will be achieved.    In Table 1 [not reproduced] are represented the processing  times (in MFLOPS) estimated by VTAP for two NAL CFD programs  (NS3D and NSMD) for this PE model, and the actual measured  MFLOPS values and VTAP-estimated MFLOPS values for the VP400 and  VP200. From this table we see that we can expect a CFD program  processing speed for the NWT PE that is 1.4 times the VP400  speed.  <H5>  4.2 NWT Hardware Configuration, Performance </H5>   Since we made it clear in 4.1 above that the PE performance  would be about 1.4 times that of the VP400, it is evident that a  NWT which satisfies (R1) can be configured with 200 or fewer PE  units. If the number of PE units is less than 200, then we are  able to adopt an interconnect network that is based either on  crossbar interconnection or complete interconnection. By  adopting a powerful network, we can expect a PE efficiency of  around 0.5. In such a case, the following would apply.    -- In order to satisfy R1, it is only necessary to have 150  or more PE units. If we have 256 MB main memory capacity for  each PE unit, then the total main memory capacity for the NWT  would be about 38 GB.    In Figure 5 is presented a conceptual block diagram of the  NWT.    <FIG ID=JPRS-JST-002C-08B>           Figure 5. NWT Conceptual Block Diagram </FIG>    <H5>  5. Features Needed in NWT Programming and Compiler (P&amp;C) </H5>   In this section we give a broad overview of the main P&amp;C  features required for maintaining an NWT PE efficiency of 0.5 or  better. These features may be implemented in program writing  (the programmer's responsibility) or by the compiler, or by a  combination of both.    In Figures 6-1 and 6-2 are given conceptual running diagrams  when programs are run on one PE and on n PE units, respectively.    <FIG ID=JPRS-JST-002C-09A>           Figure 6-1. Conceptual Diagram of Executing Job With 1 PE </FIG>      <FIG ID=JPRS-JST-002C-09B>           Figure 6-2. Conceptual Diagram of Executing Job With n PE </FIG>      S[.sup]i[/]: Execution time required for i'th sequential  process in program    SO[.sup]i[/]: OS execution time required for  S[.sup]i[/] process    SI[.sup]i[/]: OS execution time for I/O processing  required for S[.sup]i[/] process    I/O[.sup]i[/]: I/O Execution time required for  S[.sup]i[/] process    P[.sup]j[/]: Execution time when j'th parallel-processable  portion in program processed by 1 PE    PO[.sup]j[/]: OS execution time required for  P[.sup]j[/] process    PI[.sup]j[/]: OS execution time for I/O processing  required for P[.sup]j[/] process    I/O[.sup]j[/]: I/O execution time required for  P[.sup]j[/] process    ParaOS[.sup]i[/]: OS processing time that becomes  necessary to do parallel processing    P[.sup]j1[/]: When parallel-processing P[.sub]j[/],  execution time for P[.sup]j[/] portion that should be  processed by PE[.sup]1[/] (i.e. by the 1st PE)    S/RO[.sup]j1[/]: OS execution time required to S/R (data  transfer) data needed for P[.sup]j1[/] process    S/R[.sup]j1[/]: Time required to send data    PO[.sup]j1[/]: Execution time for OS processing required  for P[.sup]j1[/] process    PI[.sup]j1[/]: OS execution time for I/O processing  required for P[.sup]j1[/] process    I/O[.sup]j1[/]: I/O execution time necessary to execute  P[.sup]j1[/]    SYO[.sup]j1[/]: Execution for OS processing required to  synchronize PE[.sup]1[/] with other PEs    W[.sup]j1[/]: PE[.sub]1[/] synchronization wait time    <FIG ID=JPRS-JST-002C-09C>           Equations </FIG>       [Text]  equals Execution time when P[.sup]j[/] is parallel-processed    The time (TS) for processing the program with one PE unit  and  the time (TPN) for processing the program with n PE units are  given by the following formulas.      Now, if we take F as the number of floating point operations  (included only in P[.sup]j[/] or S[.sub]i[/]) required to  execute the entire program, we get F/TS as the program  processing speed for one PE unit, and (F/TS) x (TS/TPN) as the  program processing speed with n PE units. Accordingly, it is  essential to make TS/TPN as large as possible in the interest of  improving speed by parallel processing. The P&amp;C features  needed,  in conceptual terms, to accomplish are evident from Figures 6-1  and 6-2. The most important ones are noted below.    (A) Improved ability of P&amp;C to find those portions that  are  parallel-processable so as to shorten S[.sup]i[/] and reduce  the ParaOS[.sup]i[/] frequency    (B) Improved P&amp;C capabilities to facilitate reduction  in data  transmission volume and frequency so as to reduce frequency of  S\R[.sup]j1[/] and shorten processing time    (C) P&amp;C functions to facilitate P[.sup]j1[/] and  S\R[.sup]j1[/] parallel action    (D) P&amp;C functions to facilitate PI[.sup]j1[/] and  P[.sup]j1[/] parallel actions    (E) P&amp;C functions to facilitate reduction of volume and  frequency of referencing data stored in other PEs for the  purpose of reducing the SYO[.sup]j1[/] frequency    (F) In the interest of decreasing W[.sup]j1[/] as much as  possible,    (i) P&amp;C functions to facilitate equalization of  divisions of  P[.sup]j[/] to P[.sub]j1[/]    (ii) Since it is necessary to make it so that  S\R[.sup]j1[/] processing does not compete on the network  more than the absolute minimum, in order to make  S\R[.sup]j1[/] equal for all 1's, the necessary P&amp;C  scheduling functions    There must naturally be cooperation by both the hardware and  the computation methods to facilitate the achievement of (A)  through (F). If the degree to which (A) through (F) are achieved  rises, it will be possible to satisfy (R1) with our NWT CFD  program processing speed using P&amp;C.  References    1. Miyoshi, H.: "CFD suishin ni hitsuyona keisanki seino,"  NAL SP-13, pp 1-26, September, 1990.    2. Miyoshi, H.: "Kogiken Chokosoku suchi fudo (UHSNWT) no  koso," NAL TR-1108, May, 1991.    3. Okada, S., and Takamura, M.: "CFD muke heiretsu keisanki  no sofutouea," NAL SP-13, pp 109-116, September, 1990.</p>
		</main>
</body></html>
            